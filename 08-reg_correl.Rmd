---
title: "08 - Linear regression and correlation"
author: "Francisco E. Fonturbel"
date: "02/August/2022"
output: 
  html_document:
    includes:
        after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Linear regression

So far, we have dealt with a continuous response variable contrasted against categorical predictors. But in many cases we have continuous predictors. For example, we may assess the effect of temperature on enzymatic activity based on an experimental trial. We know from the books that many enzymes perform better at certain temperatures, and we want to test it in our laboratory. So, let's load the example file with temperature and enzymatic activity data.

```{r data}
data<-read.csv("data/08_enzactivity.csv", header = T, sep = ",")
attach(data)
head(data)
```

Before fitting a linear model, we will examine the distribution of the response variable (i.e., enzymatic activity) to assess if it follows a normal distribution:

```{r norm}
hist(enzactiv, col = "dodgerblue1")
shapiro.test(enzactiv)
```

Well, it is more or less normal...


### Linear relationships

Linear models are very common in science, as many of the relationships that we assume that variables increase or decrease along other predictors, resembling a linear function _y = ax + b_

Let's take a look of our data:

```{r plot1}
plot(enzactiv~temperature, col = "firebrick2", xlab = "Temperature (ºC)", ylab = "Enzymatic activity")
```

Well, it clearly resembles a linear relationship. But we are scientists and we assess hard data and not merely plots... let's make a linear model for it!

```{r lm}
mod1<-lm(enzactiv~temperature)
summary(mod1)
```

Well, well, well... our linear relationship is significant indeed. Enzymatic activity is positively influenced by temperature, as the books said.

But, what a linear model does? It looks for the linear function that fits better to our data. Let's plot it:

```{r plot2}
plot(enzactiv~temperature, xlab = "Temperature (ºC)", ylab = "Enzymatic activity")
abline(lm(enzactiv~temperature), col = "firebrick2", lwd = 3)
```

so, let me introduce a new concept here: residuals. The difference of each data point to the trend line is a residual. A linear regression can be assessed examining how well the resulting model fits our data. One first approach is to examine R-squared, as it gets closer to 1 the regression is optimal. In this case, our R-squared value is **`r summary(mod1)$r.squared`**, which indicates a pretty decent fit. But let's take a look on our residuals:

```{r resids}
plot(density(resid(mod1))) #A density plot
qqnorm(resid(mod1)) # A quantile normal plot - good for checking normality
qqline(resid(mod1))
```

Good, our residuals look good. In linear models, the normality of the residuals is more important than the normality of the raw variables themselves.

```{r normres}
shapiro.test(resid(mod1))
```

Great! we are good to go.


## Correlation

## Final thoughts

aaa


## Session

```{r session, echo=TRUE}
sessionInfo()
```